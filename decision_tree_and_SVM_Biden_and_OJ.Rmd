---
title: "Biden Sentamant  and OJ Trees and SVMs"
author: "Sarah Gill"
date: "2/15/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/problem-set-3")
```

```{r library}
library(readr)
library(rsample)
library(tidyverse)
#library(rcfss)
library(gbm)
library(ipred) 
library(rpart)
library(randomForest)
library(ISLR)
#library(dplyr)
```

1. Set up the data and store some things for later use:
```{r data}
set.seed(13579)
nes2008_df <- read_csv("data/nes2008.csv")

p <- subset(nes2008_df,select = -c(biden))
lambda <- seq(from = 0.0001, to = 0.04, by = 0.001)
  
```

2. Create a training set consisting of 75% of the observations, and a test set with all remaining obs. 
```{r 2}
split <- initial_split(nes2008_df, prop = 0.75)

train <- training(split)

test <- testing(split)


#nes_samples <- sample(1:nrow(nes2008_df), 
#                  nrow(nes2008_df)*0.75, 
 #                 replace = FALSE)

#train <- nes2008_df[nes_samples, ]
#test <- nes2008_df[-nes_samples, ]
```

3. Create empty objects to store training and testing MSE, and then write a loop to perform boosting on the training set with 1,000 trees for the pre-defined range of values of the shrinkage parameter, lambda.
```{r 3}
mses_train <- c()
mses_test <- c()

for (i in lambda){
  
boost.biden <- gbm(biden ~ ., 
                    data=train,
                    distribution="gaussian", #assuming distribution is gaussian
                    n.trees=1000, #do it 10000 times
                    shrinkage=i, #penalize here, this is a calssic value
                    interaction.depth = 4) #d, how many nodes to have
#deapth must be smaller than number of features
#2-5 is pretty standard for d 


#n.trees = seq(from=100,to=10000, by=100)
preds = predict(boost.biden, newdata=train, n.trees = 1000)
                 # shrinkage = lambda)

mse_train = mean((preds - train$biden)^2)
  #with(train, apply((preds - biden)^2, 2, mean))
mses_train <- append(mses_train, mse_train)

mse_test = mean((preds - test$biden)^2)
mses_test <- append(mses_test, mse_test)
}


```
 Then, plot the training set and test set MSE across shrinkage values.


```{r}

plot(lambda, mses_train,
     pch=19,
     ylab="Mean Squared Error", 
     xlab="shrinkage",
     main="Boosting Test Error")
```
```{r}
plot(lambda, mses_test,
     pch=19,
     ylab="Mean Squared Error", 
     xlab="shrinkage",
     main="Boosting Train Error")
```


4. (10 points) The test MSE values are insensitive to some precise value of lambda as long as its small enough.
Update the boosting procedure by setting lambda equal to 0.01 (but still over 1000 trees). Report the test MSE and discuss the results. How do they compare?
```{r}
boost.biden <- gbm(biden ~ ., 
                    data=train,
                    distribution="gaussian", #assuming distribution is gaussian
                    n.trees=1000, #do it 10000 times
                    shrinkage=0.01, #penalize here, this is a calssic value
                    interaction.depth = 4) #d, how many nodes to have
#deapth must be smaller than number of features
#2-5 is pretty standard for d 

preds = predict(boost.biden, newdata=train, n.trees = 1000)

mse_test_0.01 = mean((preds - test$biden)^2)
mse_test_0.01

plot(lambda, mses_train,
     pch=19,
     ylab="Mean Squared Error", 
     xlab="shrinkage",
     main="Boosting Test Error")

```

mse_test at shrinkage = 0.01 is 764.1437

```{r}
range(unlist(mses_test))
mean(unlist(mses_test))
```

Ranges from
570.0427 790.1103
with a mean of
761.7793

so 

Discuss.....


5.  Now apply bagging to the training set. What is the test set MSE for this approach?
```{r 5}
biden_bag1 <- bagging(
  formula = biden ~ .,
  data = train,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

preds = predict(biden_bag1, newdata=train)
mse_test_bag = mean((preds - test$biden)^2)
mse_test_bag
#site https://bradleyboehmke.github.io/HOML/bagging.html
```


6. Now apply random forest to the training set. What is the test set MSE for this approach?
```{r}
rf_biden <- randomForest(biden ~ ., #response featrue regressed on all the other featrues
                   data = test)

preds = predict(rf_biden, newdata=train)
mse_test_rf = mean((preds - test$biden)^2)
mse_test_rf

#same
MSE(y_pred = preds, y_true = test$biden)

#same
mean((test$biden - predict(rf_biden, train)) ^ 2)
```

report.....
682.819
Now  601.565 (different sampleing method!)

7. Now apply linear regression to the training set. What is the test set MSE for this approach?
```{r}

```


8. Compare test errors across all fits. Discuss which approach generally fits best and how you
concluded this.
```{r}

```


Support Vector Machines

1. Create a training set with a random sample of size 800, and a test set containing the remaining observations.
```{r}
#OJ
samples=sample(1:nrow(OJ),800)
train_oj <- OJ[samples, ]
test_oj <- OJ[-samples, ]

```

2. Fit a support vector classifier to the training data with cost = 0.01, with Purchase as the response and all other features as predictors. Discuss the results.
```{r}
svm_oj <- svm(Purchase ~ ., 
             data = train_oj, 
             kernel = "linear", 
             cost = 0.01,  
             scale = TRUE); summary(svm_oj) 

#table(true = OJ$Purchase, 
 #     pred = predict(svm_oj, 
  #                   newdata = train_oj))
```

discuss...

```{r}

```











