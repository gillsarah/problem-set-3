svm_oj <- svm(Purchase ~ .,
data = train_oj,
kernel = "linear",
cost = 0.01,
scale = TRUE); summary(svm_oj)
#table(true = OJ$Purchase,
#     pred = predict(svm_oj,
#                   newdata = train_oj))
table(true = train_oj$Purchase,
pred = predict(svm_oj,
newdata = train_oj))
table(true = train_oj$Purchase,
pred = predict(svm_oj,
newdata = train_oj))
table(true = train_oj$Purchase,
pred = predict(svm_oj,
newdata = train_oj))
#error rate
(1-mean(predict(svm_oj, test_oj) == test$Purchase))
mean(predict(svm_oj, test_oj) == test$Purchase)
table(pred = predict(svm_oj,
newdata = train_oj), true = train_oj$Purchase,)
table(pred = predict(svm_oj,
newdata = train_oj), true = train_oj$Purchase)
mean(predict(svm_oj, test_oj) == test$Purchase)
mean(predict(svm_oj, test_oj)
mean(predict(svm_oj, test_oj))
mean(predict(svm_oj, test_oj) == test$Purchase)
436+235+76+53
#error rate
#1 - ((correct)/(total))
1 - ((436+235)/(800))
table(pred = predict(svm_oj,
newdata = test_oj), true = test_oj$Purchase)
predict(svm_oj, test_oj) == test$Purchase
count(predict(svm_oj, test_oj) == test$Purchase)
summarise(predict(svm_oj, test_oj) == test$Purchase)
predict(svm_oj, test_oj) == test$Purchase)
1 - ((145+77)/(270))
table(pred = predict(tune_c ,
newdata = train_oj), true = train_oj$Purchase)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
144+27+20+79
145+77+29+19
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
set.seed(12345)
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
svm_oj <- svm(Purchase ~ .,
data = train_oj,
kernel = "linear",
cost = 0.01,
scale = TRUE); summary(svm_oj)
#table(true = OJ$Purchase,
#     pred = predict(svm_oj,
#                   newdata = train_oj))
table(pred = predict(svm_oj,
newdata = train_oj), true = train_oj$Purchase)
#error rate
#1 - ((correct)/(total))
1 - ((436+235)/(800))
#(1-mean(predict(svm_oj, test_oj) == test$Purchase))
table(pred = predict(svm_oj,
newdata = test_oj), true = test_oj$Purchase)
1 - ((145+77)/(270))
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
set.seed(13579)
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
set.seed(13579)
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
1-((237+432)/(800))
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
1-((146+78)/(270))
((146+78)/(270))
83+17
?MSE()
?svm()
mse_test_0.01 = MSE(predict(boost.biden, newdata=test, n.trees = 1000),test$biden))
mse_test_0.01 = MSE(predict(boost.biden, newdata=test, n.trees = 1000),test$biden)
knitr::opts_chunk$set(cache = T)
setwd("~/Documents/GitHub/problem-set-3")
library(readr)
library(rsample)
library(tidyverse)
#library(rcfss)
library(gbm)
library(ipred)
library(rpart)
library(randomForest)
library(ISLR)
library(MLmetrics)
library(e1071)
#library(dplyr)
#set.seed(13579)
nes2008_df <- read_csv("data/nes2008.csv")
p <- subset(nes2008_df,select = -c(biden))
lambda <- seq(from = 0.0001, to = 0.04, by = 0.001)
split <- initial_split(nes2008_df, prop = 0.75)
train <- training(split)
test <- testing(split)
#set.seed(13579)
mses_train <- c()
mses_test <- c()
for (i in lambda){
boost.biden <- gbm(biden ~ .,
data=train,
distribution="gaussian", #assuming distribution is gaussian
n.trees=1000, #do it 10000 times
shrinkage=i,
interaction.depth = 4) #d, how many nodes to have
preds_tr = predict(boost.biden, newdata=train, n.trees = 1000)
preds_te = predict(boost.biden, newdata=test, n.trees = 1000)
# shrinkage = lambda)
mse_train = MSE(y_pred=preds_tr, y_true=train$biden)
mses_train <- append(mses_train, mse_train)
mse_test = MSE(y_pred=preds_te, y_true=test$biden)
mses_test <- append(mses_test, mse_test)
}
plot(lambda, mses_train,
pch=19,
ylab="Mean Squared Error",
xlab="shrinkage",
main="Boosting Test Error")
plot(lambda, mses_test,
pch=19,
ylab="Mean Squared Error",
xlab="shrinkage",
main="Boosting Train Error")
#set.seed(13579)
boost.biden <- gbm(biden ~ .,
data=train,
distribution="gaussian", #assuming distribution is gaussian
n.trees=1000, #do it 10000 times
shrinkage=0.01, #penalize here, this is a calssic value
interaction.depth = 4) #d, how many nodes to have
#deapth must be smaller than number of features
#2-5 is pretty standard for d
#preds = predict(boost.biden, newdata=test, n.trees = 1000)
mse_test_0.01 = MSE(predict(boost.biden, newdata=test, n.trees = 1000), test$biden)
mse_test_0.01
range(unlist(mses_test))
mean(unlist(mses_test))
set.seed(13579)
biden_bag1 <- bagging(
formula = biden ~ .,
data = train,
nbagg = 100,
coob = TRUE,
control = rpart.control(minsplit = 2, cp = 0)
)
mse_test_bag = MSE(predict(biden_bag1, newdata=test),test$biden)
mse_test_bag
#site https://bradleyboehmke.github.io/HOML/bagging.html
set.seed(13579) #needed to set again for some reason (?)
rf_biden <- randomForest(biden ~ ., data = train)
mse_test_rf = MSE(y_pred = predict(rf_biden, newdata=test), y_true = test$biden)
mse_test_rf
#set.seed(13579) #needed to set again for some reason (?)
rf_biden <- randomForest(biden ~ ., data = train)
mse_test_rf = MSE(y_pred = predict(rf_biden, newdata=test), y_true = test$biden)
mse_test_rf
#set.seed(13579)
lm_biden <- lm(biden ~ ., data = train)
mse_lm <- mean((test$biden - predict(lm_biden, test)) ^ 2)
mse_lm
mses <- matrix(c(min(unlist(mses_test)), mse_test_0.01, mse_test_bag, mse_test_rf, mse_lm), ncol=5, byrow=TRUE)
colnames(mses) <- c( "boost(.0001-.04)", "boost(.01)","bagged","random forrest", "lm")
rownames(mses) <- c('mse')
as.table(mses)
#site: https://www.cyclismo.org/tutorial/R/types.html#tables
knitr::opts_chunk$set(cache = T)
setwd("~/Documents/GitHub/problem-set-3")
library(readr)
library(rsample)
library(tidyverse)
#library(rcfss)
library(gbm)
library(ipred)
library(rpart)
library(randomForest)
library(ISLR)
library(MLmetrics)
library(e1071)
#library(dplyr)
set.seed(13579) #global set seed did not seem to work! setting in every chunk
nes2008_df <- read_csv("data/nes2008.csv")
p <- subset(nes2008_df,select = -c(biden))
lambda <- seq(from = 0.0001, to = 0.04, by = 0.001)
set.seed(13579)
split <- initial_split(nes2008_df, prop = 0.75)
train <- training(split)
test <- testing(split)
set.seed(13579)
mses_train <- c()
mses_test <- c()
for (i in lambda){
boost.biden <- gbm(biden ~ .,
data=train,
distribution="gaussian", #assuming distribution is gaussian
n.trees=1000, #do it 10000 times
shrinkage=i,
interaction.depth = 4) #d, how many nodes to have
preds_tr = predict(boost.biden, newdata=train, n.trees = 1000)
preds_te = predict(boost.biden, newdata=test, n.trees = 1000)
# shrinkage = lambda)
mse_train = MSE(y_pred=preds_tr, y_true=train$biden)
mses_train <- append(mses_train, mse_train)
mse_test = MSE(y_pred=preds_te, y_true=test$biden)
mses_test <- append(mses_test, mse_test)
}
plot(lambda, mses_train,
pch=19,
ylab="Mean Squared Error",
xlab="shrinkage",
main="Boosting Test Error")
plot(lambda, mses_test,
pch=19,
ylab="Mean Squared Error",
xlab="shrinkage",
main="Boosting Train Error")
set.seed(13579)
boost.biden <- gbm(biden ~ .,
data=train,
distribution="gaussian", #assuming distribution is gaussian
n.trees=1000, #do it 10000 times
shrinkage=0.01, #penalize here, this is a calssic value
interaction.depth = 4) #d, how many nodes to have
#deapth must be smaller than number of features
#2-5 is pretty standard for d
#preds = predict(boost.biden, newdata=test, n.trees = 1000)
mse_test_0.01 = MSE(predict(boost.biden, newdata=test, n.trees = 1000), test$biden)
mse_test_0.01
range(unlist(mses_test))
mean(unlist(mses_test))
set.seed(13579)
biden_bag1 <- bagging(
formula = biden ~ .,
data = train,
nbagg = 100,
coob = TRUE,
control = rpart.control(minsplit = 2, cp = 0)
)
mse_test_bag = MSE(predict(biden_bag1, newdata=test),test$biden)
mse_test_bag
#site https://bradleyboehmke.github.io/HOML/bagging.html
set.seed(13579)
rf_biden <- randomForest(biden ~ ., data = train)
mse_test_rf = MSE(y_pred = predict(rf_biden, newdata=test), y_true = test$biden)
mse_test_rf
set.seed(13579)
lm_biden <- lm(biden ~ ., data = train)
mse_lm <- mean((test$biden - predict(lm_biden, test)) ^ 2)
mse_lm
mses <- matrix(c(min(unlist(mses_test)), mse_test_0.01, mse_test_bag, mse_test_rf, mse_lm), ncol=5, byrow=TRUE)
colnames(mses) <- c( "boost(.0001-.04)", "boost(.01)","bagged","random forrest", "lm")
rownames(mses) <- c('mse')
as.table(mses)
#site: https://www.cyclismo.org/tutorial/R/types.html#tables
set.seed(13579)
#OJ
samples=sample(1:nrow(OJ),800)
train_oj <- OJ[samples, ]
test_oj <- OJ[-samples, ]
set.seed(13579)
svm_oj <- svm(Purchase ~ .,
data = train_oj,
kernel = "linear",
cost = 0.01,
scale = TRUE); summary(svm_oj)
plot(svm_oj, train_oj)
table(pred = predict(svm_oj,
newdata = train_oj), true = train_oj$Purchase)
#error rate
#1 - ((correct)/(total))
1 - ((418+238)/(800))
#(1-mean(predict(svm_oj, test_oj) == test$Purchase))
table(pred = predict(svm_oj,
newdata = test_oj), true = test_oj$Purchase)
1 - ((152+76)/(270))
set.seed(13579)
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
set.seed(13579)
tune_c <- tune(svm,
Purchase ~ .,
data = train_oj,
kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
#mannually put in tuning grid: ranges
#varying costs at each fit ????
# CV errors
summary(tune_c)
# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
table(pred = predict(tuned_model,
newdata = train_oj), true = train_oj$Purchase)
1-((239+424)/(800))
table(pred = predict(tuned_model,
newdata = test_oj), true = test_oj$Purchase)
1-((152+81)/(270))
((152+81)/(270))
86+14
```{r setup}
knitr::opts_chunk$set(cache = T)
setwd("~/Documents/GitHub/problem-set-3")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/problem-set-3")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/problem-set-3")
library(readr)
library(rsample)
library(tidyverse)
library(gbm)
library(ipred)
library(rpart)
library(randomForest)
library(ISLR)
library(MLmetrics)
library(e1071)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/problem-set-3")
install.packages(c("doParallel", "ggfortify", "ggpubr"))
library(glmnet)
library(foreach)
library(pROC)
library(ggfortify)
library(ggpubr)
library(gridExtra)
library(tidyverse)
# read in the state medicaid data
dataset <- read_csv(file.choose())
dataset <- dataset %>%
select(oppose_expansion, gop_governor, percent_favorable_aca, gop_leg, bal2012, multiplier,
percent_nonwhite, percent_uninsured, percent_metro, percent_poverty)
View(dataset)
names(dataset)
#select these 10 features
#different papers said that some of these were impactful on oppose
#so we're including for all the papers (some said partisan, some said voting
#some said demographic, etc...)
dataset <- dataset %>%
select(oppose_expansion, gop_governor, percent_favorable_aca, gop_leg, bal2012, multiplier,
percent_nonwhite, percent_uninsured, percent_metro, percent_poverty)
set.seed(1244)
# store some things
#bc how penalized regn works
d <- dataset
f <- oppose_expansion ~ scale(d[,-1])
y <- d$oppose_expansion
X <- model.matrix(f, d)[,-1]
n <- length(y)
## LASSO WITH ALPHA = 1, when Alpha is 0 it's Ridge
# CV to find optimal lambda
cv1 <- cv.glmnet(X, y,
family = "binomial",
nfold = 10,
alpha = 1)
install.packages(‘glmnet’)
install.packages("glmnet")
library(dplyr)
library(tidyverse)
set.seed(123)
n <- 10000
cost <- rep(70,n)
benefit <- rnorm(n, mean=a00, sd = 20)
life <- runif(n, 30, 45)
r <- runif(n, 0.02, 0.05)
project <- cbind(cost, benefit, life, r)
project <- project %>% data.frame() %>%
setNames(c("cost", "benefit", "life", "r")) %>%
mutate(npv = (benefit - cost)/r*(1-1/(1+r)^life))
benefit <- rnorm(n, mean=100, sd = 20)
life <- runif(n, 30, 45)
r <- runif(n, 0.02, 0.05)
project <- cbind(cost, benefit, life, r)
project <- project %>% data.frame() %>%
setNames(c("cost", "benefit", "life", "r")) %>%
mutate(npv = (benefit - cost)/r*(1-1/(1+r)^life))
mean(project$npv)
sd(project$npv)
#probablility NPV is neg
sum(project$npv <0)/n
#plot:
hist(project$npv, 100, main = "Historgram 1 - No Upfront Cost",
xlab = "Present Value(in mill)")
#triangular dist: rtri()
?rtri
#triangular dist: rtri()
?rtri()
#triangular dist: rtri()
?dtri()
install.packages("traingle")
library(triangle)
